"""of Sentiment Analysis - Group 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13D7AaTT--yI-URkn2rMOzEsINrEWSA6a


"""
import os
os.system('pip install nltk')
import nltk
import requests
from nltk.tokenize import word_tokenize
import pandas as pd
import numpy as np
import string
import streamlit as st
import re



df = pd.read_csv('dataset -P582.csv')




# --- STEP 1: DEFINE AUGMENTED STOP WORDS AND MAPPING DICTIONARY ---

# 1. Common Romanized Hindi Function Words (Noise)
hindi_noise_words = [
    'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
    'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
    'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
    'kya', 'tera', 'mera', 'diya', 'diye', 'de'
]

# 2. Key Romanized Sentiment/Functional Word Mapping
romanized_mapping = {
    'accha': 'good',
    'acha': 'good',
    'achha': 'good',
    'bahut': 'very',
    'bhut': 'very',
    'kharab': 'bad',
    'bekar': 'bad',
    'sahi': 'correct',
    'nhi': 'not',
    'nahi': 'not',
    'milega': 'receive',
    'mila': 'received',
    'liya': 'take',
    'diya': 'give',
    'lene': 'take',
    'achcha':'good',
    'achhi':'good',
    'achchaa':'good',
    'acchi':'good',
    'aap':'you',
    'bakwaas':'bad',
    'bakwas':'bad',
    'bada':'big'
}

# 3. Download NLTK resources
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

# 4. Define the URL of English stopwords file on your GitHub
STOPWORDS_URL = "https://github.com/manishmaltare/Manish-Maltare/blob/main/english_stopwords.txt"

# Function to load stopwords from GitHub
def load_stopwords_from_github():
    response = requests.get(STOPWORDS_URL)
    if response.status_code == 200:
        stopwords_list = response.text.splitlines()
        return set(stopwords_list)
    else:
        raise Exception(f"Failed to load stopwords from GitHub: {response.status_code}")

# Load English stopwords
english_stopwords = load_stopwords_from_github()

# List of common negation words to add
negation_words = {
    'no', 'not', 'nor', "n't", 'never', 'none', 'nothing', 'nowhere',
    'neither', 'cannot', 'can\'t', 'don\'t', 'doesn\'t', 'didn\'t',
    'won\'t', 'wouldn\'t', 'shouldn\'t', 'isn\'t', 'aren\'t', 'ain\'t',
    'without', 'barely', 'hardly', 'rarely', 'seldom'
}

# Combine English stopwords and Hindi noise words but EXCLUDE negation words
combined_stopwords = english_stopwords.union(set(hindi_noise_words))

# Remove negation words from stopwords to keep them in text
stop_words = combined_stopwords.difference(negation_words)

# Negation words are kept separately for explicit handling during preprocessing
# You can use this list to implement negation scope detection or feature engineering


# --- STEP 2: MODIFIED CLEANING FUNCTION ---

def vectorized_clean_series(s: pd.Series):
    s = s.fillna('').astype(str)
    s = s.str.normalize('NFKC')
    s = s.str.lower()

    # initial cleanup (URLs, emails, etc.)
    s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
    s = s.str.replace(r'\S+@\S+', ' ', regex=True)
    s = s.str.replace(r'<.*?>', ' ', regex=True)
    s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)

    # romanized word mapping
    def replace_romanized_words(text):
        if not text:
            return ""
        words = text.split()
        words = [romanized_mapping.get(word, word) for word in words]
        return ' '.join(words)

    s = s.apply(replace_romanized_words)

    # remove non alpha except spaces
    s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
    s = s.str.replace(r'\s+', ' ', regex=True).str.strip()

    # keep tokens that are NOT stopwords or that ARE negation words
    s = s.apply(lambda x: ' '.join([
        word for word in word_tokenize(x)
        if (word not in stop_words) or (word in negation_words)
    ]))

    return s

# Create a new DataFrame with cleaned columns
df_new_2 = df.copy()

df_new_2['title'] = vectorized_clean_series(df_new_2['title'])
df_new_2['body']  = vectorized_clean_series(df_new_2['body'])

df_new_2['rating'] = df['rating']

df_new_2['title_text_length'] = df_new_2['title'].apply(lambda x: len(str(x).split()))

df_new_2['body_text_length'] = df_new_2['body'].apply(lambda x: len(str(x).split()))

# Refined lexicon loader ‚Äì uses first float after word as score
def load_lexicon(filename):
    lexicon = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) > 1:
                word, score = parts[0], parts[1]
                try:
                    lexicon[word] = float(score)
                except ValueError:
                    continue  # Skip malformed lines
    return lexicon

# Load the lexicon from the attached file
sentiment_lexicon = load_lexicon("lexicon.txt")

# Text preprocessing: lowercase, remove punctuation, strip spaces
def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text.strip()

# Get sentiment score from the lexicon, default 0
def get_sentiment_score(word):
    return sentiment_lexicon.get(word, 0)

# Compound sentiment score for input text
def calculate_compound_score_with_negation(text, negation_words=set()):
    tokens = preprocess_text(text)
    compound_score = 0.0
    negate_scope = 0  # number of words to negate after negation word
    NEGATION_WINDOW = 3  # number of words to invert after negation

    for token in tokens:
        score = get_sentiment_score(token)

        # If current token is a negation word, enable negation window
        if token in negation_words:
            negate_scope = NEGATION_WINDOW
            continue  # negation words themselves usually no polarity score

        # If inside negation scope, invert polarity
        if negate_scope > 0:
            score = -score
            negate_scope -= 1

        compound_score += score

    return compound_score


# Sentiment category from score
def get_sentiment_category(text):
    compound_score = calculate_compound_score_with_negation(text, negation_words)
    if compound_score >= 0.05:
        return "Positive"
    elif compound_score <= -0.05:
        return "Negative"
    else:
        return "Neutral"


# Assuming df_new_2['body'] is your input column
df_new_2['body_sentiments'] = df_new_2['body'].apply(get_sentiment_category)
print(df_new_2[['body', 'body_sentiments']])

df_new_2[['body_sentiments']].value_counts()



rating_map = {
    1: 'a',
    2: 'b',
    3: 'c',
    4: 'd',
    5: 'e',}

df_new_2['rating']=df_new_2['rating'].map(rating_map)
x = df_new_2[['title', 'body','rating']]
y = df_new_2['body_sentiments']



x_train = x.iloc[0:1008]
x_test = x.iloc[1008:]
y_train = y.iloc[0:1008]
y_test = y.iloc[1008:]

x_train['combined'] = x_train['title']+x_train['rating']+x_train['body']
x_test['combined'] = x_test['title']+x_test['rating']+x_test['body']

def preprocess(text):
    if not isinstance(text, str):
        return []
    text = text.lower().translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    return text.split()
import math
from collections import defaultdict

def build_vocab_idf(corpus):
    vocab = {}
    doc_freq = defaultdict(int)
    processed_docs = []

    for doc in corpus:
        tokens = preprocess(doc)
        processed_docs.append(tokens)
        unique_tokens = set(tokens)
        for token in unique_tokens:
            doc_freq[token] += 1

    vocab = {word: idx for idx, word in enumerate(sorted(doc_freq))}
    total_docs = len(corpus)
    idf = {word: math.log((total_docs + 1) / (doc_freq[word] + 1)) + 1 for word in vocab}

    return vocab, idf, processed_docs


def compute_tfidf(processed_docs, vocab, idf):
    tfidf_matrix = []

    for tokens in processed_docs:
        tf = defaultdict(int)
        for token in tokens:
            tf[token] += 1

        total_tokens = len(tokens)
        doc_vector = [0.0] * len(vocab)

        for token in tokens:
            if token in vocab:
                tf_val = tf[token] / total_tokens
                tfidf_val = tf_val * idf[token]
                doc_vector[vocab[token]] = tfidf_val

        tfidf_matrix.append(doc_vector)

    return np.array(tfidf_matrix)

# 1. Create 'combined' column if not already created
x_train['combined'] = x_train['title'] + x_train['rating'] + x_train['body']
x_test['combined'] = x_test['title'] + x_test['rating'] + x_test['body']

# 2. Prepare text data
train_corpus = x_train['combined'].astype(str).tolist()
test_corpus = x_test['combined'].astype(str).tolist()

# 3. Build vocabulary and IDF from training corpus only
vocab, idf, processed_train_docs = build_vocab_idf(train_corpus)

# 4. Vectorize both training and test data
x_train_vectorized = compute_tfidf(processed_train_docs, vocab, idf)

# Process test docs separately using the same vocab and idf
processed_test_docs = [preprocess(doc) for doc in test_corpus]
x_test_vectorized = compute_tfidf(processed_test_docs, vocab, idf)

# 5. Check shapes
print("Train TF-IDF shape:", x_train_vectorized.shape)
print("Test TF-IDF shape:", x_test_vectorized.shape)



import math
from collections import defaultdict

### 1. Preprocessing
def preprocess(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    return text.split()

### 2. Build vocabulary and calculate IDF
def build_vocab_and_idf(corpus):
    doc_count = len(corpus)
    vocab = {}
    doc_freq = defaultdict(int)
    processed_docs = []

    for doc in corpus:
        tokens = preprocess(doc)
        processed_docs.append(tokens)
        unique_tokens = set(tokens)
        for token in unique_tokens:
            doc_freq[token] += 1

    # Build vocab: word -> index
    vocab = {word: idx for idx, word in enumerate(sorted(doc_freq))}

    # Compute IDF
    idf = {word: math.log(doc_count / (1 + doc_freq[word])) for word in vocab}

    return vocab, idf, processed_docs


def compute_tfidf(processed_docs, vocab, idf):
    tfidf_matrix = []

    for tokens in processed_docs:
        tf = defaultdict(int)
        for token in tokens:
            tf[token] += 1

        # Build TF-IDF vector
        doc_vector = [0.0] * len(vocab)
        for token in tokens:
            if token in vocab:
                tf_val = tf[token] / len(tokens)
                tfidf_val = tf_val * idf[token]
                doc_vector[vocab[token]] = tfidf_val
        tfidf_matrix.append(doc_vector)

    return np.array(tfidf_matrix)


train_texts = x_train['combined'].tolist()
test_texts = x_test['combined'].tolist()
all_texts = train_texts + test_texts

# Build vocab & IDF from all data
vocab, idf, all_processed = build_vocab_and_idf(all_texts)

# Split processed docs back into train/test
processed_train = all_processed[:len(train_texts)]
processed_test = all_processed[len(train_texts):]

# Compute TF-IDF vectors
x_train_vectorized = compute_tfidf(processed_train, vocab, idf)
x_test_vectorized = compute_tfidf(processed_test, vocab, idf)



import pickle
from sklearn.svm import SVC

# Assuming you already have x_train_vectorized and y_train

# Train the model (as you've already done)
sv_train = SVC(kernel='linear', gamma='scale', degree=2, C=4.6415888336127775, probability=True)
model_train_tuned = sv_train.fit(x_train_vectorized, y_train)


import string
from collections import defaultdict
import streamlit as st

# Save the trained model to a .pkl file
with open("svm_model.pkl", "wb") as f:
    pickle.dump(model_train_tuned, f)


# Load your trained model
with open('svm_model.pkl', 'rb') as f:
    model = pickle.load(f)


# Load or rebuild your vocab and idf here; they must match the training version
# Example:
# vocab = loaded_vocab
# idf = loaded_idf

def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text.split()

def compute_tfidf(text_list, vocab, idf):
    tfidf_matrix = []
    for text in text_list:
        tokens = preprocess_text(text)
        tf = defaultdict(int)
        for token in tokens:
            tf[token] += 1
        total_tokens = len(tokens)
        doc_vector = np.zeros(len(vocab))
        for token in tokens:
            if token in vocab:
                tf_val = tf[token] / total_tokens
                tfidf_val = tf_val * idf[token]
                doc_vector[vocab[token]] = tfidf_val
        tfidf_matrix.append(doc_vector)
    return np.array(tfidf_matrix)

def predict_sentiment_with_proba(text_list):
    tfidf_feat = compute_tfidf(text_list, vocab, idf)
    probas = model.predict_proba(tfidf_feat)
    class_labels = model.classes_
    return probas, class_labels

# Streamlit app
# At the top of your Streamlit app UI code:
st.title("Sentiment Analysis üòäüòêüòû")
st.subheader("SVC model - Get sentiment probabilities with confidence levels")

text_input = st.text_area("Enter text (one or more lines)", height=150)

if st.button("Analyze Sentiment"):
    if text_input:
        raw_lines = [line.strip() for line in text_input.split('\n') if line.strip()]
        cleaned_lines = vectorized_clean_series(pd.Series(raw_lines)).tolist()
        probabilities, classes = predict_sentiment_with_proba(cleaned_lines)

        for i, line in enumerate(raw_lines):
            st.write(f"Input: **{line}**")
            
            # Find the index of highest probability for this input
            max_idx = probabilities[i].argmax()
            
            for idx, (label, prob) in enumerate(zip(classes, probabilities[i])):
                if idx == max_idx:
                    st.write(f"- **{label}: {prob*100:.2f}%**")
                else:
                    st.write(f"- {label}: {prob*100:.2f}%")
            st.write("---")
    else:
        st.write("Please enter text.")

