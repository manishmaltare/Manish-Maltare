# -*- coding: utf-8 -*-
"""ASSIGNMENT - LogisticRegression - Deployment - Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gmPcc_b1ChktZVuTt6Vm1lihRKaTwxpL
"""

import pandas as pd # LOAD DATASET

features = pd.read_csv('Titanic_train.csv')

target = pd.read_csv('Titanic_test.csv')

# FEATURE EXTRACTION - i did perform it before EDA- 1 as the feature & target variable are given
# separately.

features.shape

target.shape

features.isnull().sum()

# DATA PREPROCESSING : - "Imputation"
# Imputation i am using here to fill the blank rows, keeping rows empty can lead to
# model not run.
# I am using imputation here by filling the 'age' column with mean value of overall rows.

features['Age']=features['Age'].fillna(features['Age'].mean())

features.isnull().sum()

# i am using imputation by applying most frequent rows into the blank ones in
# Embarked, as its having only 2-rows empty.

features['Embarked']=features['Embarked'].fillna(features['Embarked'].mode()[0])

features.isnull().sum()

# i am using imputation here, by first separating first letter of Cabin,
# beacause, it is deck no. which i feel is the only useful for process.
# After that i did put a random string 'U' in the null rows.

features['Cabin']=features['Cabin'].str[0]

features['Cabin']

features['Cabin']=features['Cabin'].fillna('U')

features['Cabin']

features.isnull().sum()

features.columns

features.nunique()

# I drop the columns PassengerID & Name.
# Main reason : - having data value in all rows unique, Regression will not work.
# PassengerID is just serial no. as per the registration. and in +1 incerasing order.
# Name - i believe would not matter, as there are string data, which is not eay to convert
# into continuous no. or binary

features=features.drop(['PassengerId','Name'],axis=1)

features.head()

# DATA PREPROCESSING : - "Encode categorical data"

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

features['Sex']=le.fit_transform(features['Sex'])

features.head()

features['Embarked']=le.fit_transform(features['Embarked'])

features.head()

features.nunique()

# removing ticket as its having 76% on unique values as string & integers.
features=features.drop(['Ticket'],axis=1)

features['Cabin']=le.fit_transform(features['Cabin'])

features.head()

target.head()

target.shape

target.nunique()

target.columns

target=target.drop(['PassengerId','Name','Ticket'],axis=1)

target.head()

target.isnull().sum()

# Repeating the same steps of Data preprocessing for 'target' varable to prepare the dataset for prediction.

target['Age']=target['Age'].fillna(target['Age'].mean())

target.isnull().sum()

target['Cabin']=target['Cabin'].str[0]

target.head()

target['Cabin']=target['Cabin'].fillna('U')

target.head()

# merging 'features' & 'target' variables to perform EDA.
# EDA-1

data_merged=pd.concat([features,target],ignore_index=True)
data_merged.head()

data_merged.columns

data_merged.shape

data_merged.size

import seaborn as sns
import matplotlib.pyplot as plt

z=data_merged.corr(numeric_only=True)

sns.heatmap(z,annot=True,annot_kws={'size': 12})
plt.show()
# Here the map shows that only Parch & Fare are positively coreleated, with Survived.
# which indicates that survival rate increases with no of parents & fare.
# Pclass is higly inversly corelated as comapre to Age & SibSp.
# which indicates that 1st class passenges have more chances of survival, as total count of 1st class is 216 & 3rd class is 1473.
# SibSp & Parch are mutully corelated variables.
# Fare & Survival comes in 2nd place.
# which indicates that with increase of fair , survival rate increases.

data_merged.columns

sns.pairplot(data_merged)

sns.boxplot(data_merged)
# Fare has too many outliers, indicates the people with
# high fares (1st & 2nd class) are very low in no. as median is also low to 10-15
# Age shows that elderly people are very less (as outliers)
# The median likely to be between 25-35 years old

sns.displot(data_merged)



data_merged.info()

data_merged.describe()
# Age , the average is 29.88 years. Max is 88 years old.
# Fare, Max. fare price is 512, avg is 33.29 units.
# Pclass, mean is 2.29 indicates that most people are of middle income group (2nd class).
# 75% people are from calss 3rd, which means majority people travelling are of class 3rd.

target['Sex']=le.fit_transform(target['Sex'])

target['Embarked']=le.fit_transform(target['Embarked'])

target['Cabin']=le.fit_transform(target['Cabin'])

target.head()

x = features.drop(['Survived'],axis=1)
x.head()

x.columns

y= features['Survived']
y.head()

# EDA-2 (MODEL BUILDING)
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,train_size=0.8)

x_train.shape,x_test.shape,y_train.shape,y_test.shape

column_names=x_train.columns
column_names

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

x_train=sc.fit_transform(x_train)

x_test=sc.fit_transform(x_test)

from sklearn.linear_model import LogisticRegression

model=LogisticRegression()

model.fit(x_train,y_train)

scaler1 = sc.fit_transform(x_train)

# 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'
scaler = pd.DataFrame(scaler1, columns=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'])

joblib.dump(model,'logistic_reg_model.pkl') # Creating a PKL file for deployment later

joblib.dump(scaler,'scaler.pkl')

y_pred=model.predict(x_train)

# MODEL EVALUATION
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_train,y_pred)
cm
# TP = 383 (correctly predicted survived)
# TN = 185 (correctly predicted not-survived)
# FP = 61 (incorrectly predicted survived)
# FN = 83 (incorrectly predicted not-survived)

from sklearn.metrics import classification_report

cr=classification_report(y_pred,y_train)

print(cr)
# Training accuracy or F1-Score is 80%, which means its a good model.
# not survived are = 466
# survived are = 246

import statsmodels.api as sm

X=sm.add_constant(x_train)

reasult=model.fit(x_train,y_train)

logit_model=sm.Logit(y_train,X)

reasult=logit_model.fit()

print(reasult.summary())
# Overall Pseudo R - square value is 32% which is above 30%, implies that the model is moderately accurate
# Total 4- varibale are significant having values < 0.05 (p-values)
# which are : Pclass, Sex, Age,Sibsp
# Embarked variable is moderately significant.
# Fare is highly insignigficant, followed by Parch & Cabin - moderately insignificant

column_names

pd.DataFrame({'Column0': ['x1','x2','x3','x4','x5','x6','x7','x8'],'Column':column_names, 'Values': model.coef_.reshape(-1)})

# CHECKING TEST ACCURACY OF MODEL

model1=LogisticRegression()

model1=model1.fit(x_test,y_test)
model1

y_pred1=model1.predict(x_test)

y_pred1

cm1=confusion_matrix(y_pred1,y_test)
cm1
# TP = 92 (correctly predicted survived)
# TN = 58 (correctly predicted not-survived)
# FP = 16 (incorrectly predicted survived)
# FN = 13 (incorrectly predicted not-survived)

cr1=classification_report(y_pred1,y_test)

print(cr1) # Testing accuracy F1 - Score is 84%
# Testing model

target.head()

features.head()

target.isnull().sum()

target['Fare'].head()

target['Fare'].tail()

target['Fare'].iloc[152]

target.shape

target=target.drop([152])

target['Fare'].iloc[152]

target.shape

from sklearn.metrics import roc_curve, auc

# PREDICTION OF TARGET VARIABLES (using file "Titanic_test.csv" )
y_pred_new=model.predict(target)
y_pred_new

# ROC - AUC SCORE
y_prob=model.predict_proba(x_test)[:,1]
y_prob

y_prob.shape

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

fpr, tpr, threshold = roc_curve(y_test,y_prob)

roc_auc=auc(fpr, tpr)
print(roc_auc*100)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
# AUC Accuracy is 88%,
# the rate of True positive is high and False positive is low.
# as the curve is above diagonal line, means the model is perfect.

# CROSS VALIDATION
from sklearn.model_selection import KFold,cross_val_score

kf=KFold(n_splits=10,shuffle=True, random_state=42)

score = cross_val_score(model,x,y,cv=kf,scoring='accuracy')

print(score.mean()*100)
# Cross validation score is = 79%

# Deployment :

model

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

import streamlit as st
import pandas as pd
import joblib
import numpy as np

# ------------------- PAGE CONFIG -------------------
st.set_page_config(
    page_title="Titanic Survival Prediction App",
    layout="centered"
)

# ------------------- CACHING MODEL -------------------
# Load model and scaler once and reuse across sessions
@st.cache_resource
def load_model_and_scaler():
    try:
        return (
            joblib.load('logistic_reg_model.pkl'),
            joblib.load('scaler.pkl')
        )
    except FileNotFoundError as e:
        st.error(f"⚠️ Missing file: {e}. Ensure 'logistic_reg_model.pkl' and 'scaler.pkl' are present.")
        st.stop()

model, scaler = load_model_and_scaler()

# ------------------- MAPPINGS -------------------
EMBARKED_MAPPING = {0: 'C', 1: 'Q', 2: 'S'}
CABIN_MAPPING = {0: 'U', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'T'}

# ------------------- TITLE -------------------
st.title("🚢 Titanic Survival Prediction App")
st.write("Fill in the passenger details below to get the prediction:")

# ------------------- INPUT LAYOUT -------------------
col1, col2 = st.columns(2)

with col1:
    pclass = st.selectbox('Pclass', [1, 2, 3],
                          format_func=lambda x: f"{x} ({'First' if x==1 else 'Second' if x==2 else 'Third'})")
    age = st.number_input('Age', 0, 100, 29)
    parch = st.number_input('Parch', 0, 10, 0, help="Number of Parents/Children Aboard")
    embarked = st.selectbox('Embarked', EMBARKED_MAPPING.keys(),
                            format_func=lambda x: EMBARKED_MAPPING[x])

with col2:
    sex = st.selectbox('Sex', [0, 1], format_func=lambda x: 'Female' if x == 0 else 'Male')
    sibsp = st.number_input('SibSp', 0, 10, 0, help="Number of Siblings/Spouses Aboard")
    fare = st.number_input('Fare', 0.0, 600.0, 32.2)
    cabin = st.selectbox('Cabin', CABIN_MAPPING.keys(), format_func=lambda x: CABIN_MAPPING[x])

# ------------------- PREDICTION FUNCTION -------------------
@st.cache_data(show_spinner=False)
def predict_survival(features):
    """Predict survival with scaled input."""
    input_scaled = scaler.transform(pd.DataFrame([features],
                                columns=['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked']))
    prob = model.predict_proba(input_scaled)[0][1]
    return int(prob >= 0.5), prob

# ------------------- BUTTON ACTION -------------------
if st.button('Predict Survival'):
    with st.spinner('Predicting...'):
        features = [pclass, sex, age, sibsp, parch, fare, cabin, embarked]
        prediction, probability = predict_survival(features)

    st.write("---")
    if prediction:
        st.success(f"🎉 Likely to **SURVIVE**! Probability: {probability:.2%}")
    else:
        st.error(f"❌ Not likely to survive. Probability: {probability:.2%}")

