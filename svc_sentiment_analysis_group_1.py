# -*- coding: utf-8 -*-
"""SVC - Sentiment Analysis - Group 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tc8b4PulCxv8_XkjjgYkKbXCkb1N4L5d

### Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

"""### Dataset Loading"""

df = pd.read_excel('dataset -P582.xlsx')

"""### Data Pre-processing"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd
import re

# --- STEP 1: DEFINE AUGMENTED STOP WORDS AND MAPPING DICTIONARY ---

# 1. Common Romanized Hindi Function Words (Noise)
# Add these to your English stopwords list.
hindi_noise_words = [
    'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
    'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
    'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
    'kya', 'tera', 'mera', 'diya', 'diye', 'de'
]

# 2. Key Romanized Sentiment/Functional Word Mapping
# This translates key Hinglish words to their English equivalents.
romanized_mapping = {
    'accha': 'good',
    'acha': 'good',
    'achha': 'good',
    'bahut': 'very',
    'bhut': 'very',
    'kharab': 'bad',
    'bekar': 'bad',
    'sahi': 'correct',
    'nhi': 'not',
    'nahi': 'not',
    'milega': 'receive',
    'mila': 'received',
    'liya': 'take',
    'diya': 'give',
    'lene': 'take',
    'achcha':'good',
    'achhi':'good',
    'achchaa':'good',
    'acchi':'good',
    'aap':'you',
    'bakwaas':'bad',
    'bakwas':'bad',
    'bada':'big'
}

# 3. Download resources and create the final augmented stop_words set
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True) # Add this line to download punkt_tab

english_stopwords = set(stopwords.words('english'))
stop_words = english_stopwords.union(set(hindi_noise_words))


# --- STEP 2: MODIFIED CLEANING FUNCTION ---

def vectorized_clean_series(s: pd.Series):
    s = s.fillna('').astype(str)
    s = s.str.normalize('NFKC')
    s = s.str.lower()

    # 1. Initial cleanup (URLs, emails, HTML, numbers)
    s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
    s = s.str.replace(r'\S+@\S+', ' ', regex=True)
    s = s.str.replace(r'<.*?>', ' ', regex=True)
    s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)

    # 2. HINGLISH MAPPING (Must happen before punctuation/tokenization)

    def replace_romanized_words(text):
        if not text:
            return ""
        # The text is already lower-cased from a previous step
        words = text.split()

        # Replace words based on the dictionary
        words = [romanized_mapping.get(word, word) for word in words]

        return ' '.join(words)

    s = s.apply(replace_romanized_words)

    # 3. Final structural cleanup (Punctuation and whitespace)
    s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
    s = s.str.replace(r'\s+', ' ', regex=True).str.strip()

    # 4. Tokenize and remove augmented stopwords

    s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))

    return s

# Create a new DataFrame with cleaned columns
df_new_2 = df.copy()

df_new_2['title'] = vectorized_clean_series(df_new_2['title'])
df_new_2['body']  = vectorized_clean_series(df_new_2['body'])

df_new_2['rating'] = df['rating']

df_new_2['title_text_length'] = df_new_2['title'].apply(lambda x: len(str(x).split()))

df_new_2['body_text_length'] = df_new_2['body'].apply(lambda x: len(str(x).split()))

"""### Conversion of text data into vectors for Model Building"""

# Install vaderSentiment if not already installed
!pip install vaderSentiment

# Import the SentimentIntensityAnalyzer from the vaderSentiment library
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Create an instance of the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()


# Create an instance of the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def get_vader_sentiment_category(text):
    if not isinstance(text, str):
        return "Neutral"  # Default to Neutral for non-string inputs
    vs = analyzer.polarity_scores(text)
    compound_score = vs['compound']
    if compound_score >= 0.05:
        return "Positive"
    elif compound_score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

df_new_2['body_sentiments']=df_new_2['body'].apply(get_vader_sentiment_category)

df_new_2.columns

df_new_2['rating'].value_counts()

"""### Imputing : text into numercial"""

rating_map = {
    1: 'a',
    2: 'b',
    3: 'c',
    4: 'd',
    5: 'e',}

df_new_2['rating']=df_new_2['rating'].map(rating_map)
x = df_new_2[['title', 'body','rating']]
y = df_new_2['body_sentiments']

"""# **SVC - by Manish**

## Model Selection
"""

from sklearn.model_selection import train_test_split, RandomizedSearchCV
x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42, test_size = 0.2)
x_train['combined'] = x_train['title']+x_train['rating']+x_train['body']
x_test['combined'] = x_test['title']+x_test['rating']+x_test['body']

"""### Vectorization using TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
x_train_vectorized = tfidf.fit_transform(x_train['combined'])
x_test_vectorized = tfidf.fit_transform(x_test['combined'])
from sklearn.svm import SVC
sv = SVC()

SVC_Model = SVC(kernel='linear',gamma='scale',degree=2,C=4.6415888336127775)

"""# Task
Convert the provided notebook code into a Python script (`.py` file) for a Streamlit application that predicts sentiment (Positive, Negative, Neutral) and displays sentiment percentages based on user input of title, rating, and body.

## Extract and refactor code

### Subtask:
Identify and extract the necessary code for data cleaning, vectorization, and model inference from the notebook.

**Reasoning**:
Extract the relevant code for data cleaning, vectorization, and model from the notebook and save it into `app.py`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import SVC
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import pickle
# 
# # --- Data Cleaning Code ---
# hindi_noise_words = [
#     'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
#     'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
#     'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
#     'kya', 'tera', 'mera', 'diya', 'diye', 'de'
# ]
# 
# romanized_mapping = {
#     'accha': 'good',
#     'acha': 'good',
#     'achha': 'good',
#     'bahut': 'very',
#     'bhut': 'very',
#     'kharab': 'bad',
#     'bekar': 'bad',
#     'sahi': 'correct',
#     'nhi': 'not',
#     'nahi': 'not',
#     'milega': 'receive',
#     'mila': 'received',
#     'liya': 'take',
#     'diya': 'give',
#     'lene': 'take',
#     'achcha':'good',
#     'achhi':'good',
#     'achchaa':'good',
#     'acchi':'good',
#     'aap':'you',
#     'bakwaas':'bad',
#     'bakwas':'bad',
#     'bada':'big'
# }
# 
# nltk.download('stopwords', quiet=True)
# nltk.download('punkt', quiet=True)
# nltk.download('punkt_tab', quiet=True)
# 
# english_stopwords = set(stopwords.words('english'))
# stop_words = english_stopwords.union(set(hindi_noise_words))
# 
# def vectorized_clean_series(s: pd.Series):
#     s = s.fillna('').astype(str)
#     s = s.str.normalize('NFKC')
#     s = s.str.lower()
# 
#     s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
#     s = s.str.replace(r'\S+@\S+', ' ', regex=True)
#     s = s.str.replace(r'<.*?>', ' ', regex=True)
#     s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)
# 
#     def replace_romanized_words(text):
#         if not text:
#             return ""
#         words = text.split()
#         words = [romanized_mapping.get(word, word) for word in words]
#         return ' '.join(words)
# 
#     s = s.apply(replace_romanized_words)
# 
#     s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
#     s = s.str.replace(r'\s+', ' ', regex=True).str.strip()
# 
#     s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))
# 
#     return s
# 
# # --- VADER Sentiment Analysis ---
# analyzer = SentimentIntensityAnalyzer()
# 
# def get_vader_sentiment_category(text):
#     if not isinstance(text, str):
#         return "Neutral"
#     vs = analyzer.polarity_scores(text)
#     compound_score = vs['compound']
#     if compound_score >= 0.05:
#         return "Positive"
#     elif compound_score <= -0.05:
#         return "Negative"
#     else:
#         return "Neutral"
# 
# # --- Model and Vectorization Code ---
# # Assuming x_train and y_train are available for fitting
# # In a real Streamlit app, you would load a pre-trained model and vectorizer
# 
# # Placeholder for fitting - in the actual app, you'd load these
# # tfidf = TfidfVectorizer()
# # x_train_vectorized = tfidf.fit_transform(x_train['combined'])
# # SVC_Model = SVC(kernel='linear',gamma='scale',degree=2,C=4.6415888336127775)
# # SVC_Model.fit(x_train_vectorized, y_train)
# 
# # To make this script runnable for the next steps, we need to
# # simulate loading a trained model and vectorizer.
# # In a real scenario, you would train the model and save it with pickle.
# # For this subtask, we will just define the objects.
# 
# tfidf = TfidfVectorizer()
# SVC_Model = SVC(kernel='linear',gamma='scale',degree=2,C=4.6415888336127775)
# 
# # We will save dummy objects to app.py so that the next step can load them
# # In a real application, you would train the model and vectorizer on your data
# # and save the trained objects.
# with open('tfidf_vectorizer.pkl', 'wb') as f:
#     pickle.dump(tfidf, f)
# 
# with open('svc_model.pkl', 'wb') as f:
#     pickle.dump(SVC_Model, f)

"""## Create streamlit app structure

### Subtask:
Write the Python code for the Streamlit application, including input fields for title, rating, and body, and a button to trigger sentiment prediction.

**Reasoning**:
Add the necessary Streamlit code to the `app.py` file, including input fields and a button for user interaction.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import SVC
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import pickle
# import streamlit as st
# 
# # --- Data Cleaning Code ---
# hindi_noise_words = [
#     'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
#     'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
#     'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
#     'kya', 'tera', 'mera', 'diya', 'diye', 'de'
# ]
# 
# romanized_mapping = {
#     'accha': 'good',
#     'acha': 'good',
#     'achha': 'good',
#     'bahut': 'very',
#     'bhut': 'very',
#     'kharab': 'bad',
#     'bekar': 'bad',
#     'sahi': 'correct',
#     'nhi': 'not',
#     'nahi': 'not',
#     'milega': 'receive',
#     'mila': 'received',
#     'liya': 'take',
#     'diya': 'give',
#     'lene': 'take',
#     'achcha':'good',
#     'achhi':'good',
#     'achchaa':'good',
#     'acchi':'good',
#     'aap':'you',
#     'bakwaas':'bad',
#     'bakwas':'bad',
#     'bada':'big'
# }
# 
# # Download resources only if they haven't been downloaded
# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt_tab')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt_tab', quiet=True)
# 
# 
# english_stopwords = set(stopwords.words('english'))
# stop_words = english_stopwords.union(set(hindi_noise_words))
# 
# def vectorized_clean_series(s: pd.Series):
#     s = s.fillna('').astype(str)
#     s = s.str.normalize('NFKC')
#     s = s.str.lower()
# 
#     s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
#     s = s.str.replace(r'\S+@\S+', ' ', regex=True)
#     s = s.str.replace(r'<.*?>', ' ', regex=True)
#     s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)
# 
#     def replace_romanized_words(text):
#         if not text:
#             return ""
#         words = text.split()
#         words = [romanized_mapping.get(word, word) for word in words]
#         return ' '.join(words)
# 
#     s = s.apply(replace_romanized_words)
# 
#     s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
#     s = s.str.replace(r'\s+', ' ', regex=True).str.strip()
# 
#     s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))
# 
#     return s
# 
# # --- VADER Sentiment Analysis ---
# analyzer = SentimentIntensityAnalyzer()
# 
# def get_vader_sentiment_category(text):
#     if not isinstance(text, str):
#         return "Neutral"
#     vs = analyzer.polarity_scores(text)
#     compound_score = vs['compound']
#     if compound_score >= 0.05:
#         return "Positive"
#     elif compound_score <= -0.05:
#         return "Negative"
#     else:
#         return "Neutral"
# 
# # --- Model and Vectorization Loading ---
# # Load the pre-trained TF-IDF vectorizer and SVC model
# # In a real application, you would train and save these first.
# try:
#     with open('tfidf_vectorizer.pkl', 'rb') as f:
#         tfidf = pickle.load(f)
# except FileNotFoundError:
#     st.error("TF-IDF vectorizer file not found. Please ensure 'tfidf_vectorizer.pkl' is in the same directory.")
#     st.stop()
# 
# try:
#     with open('svc_model.pkl', 'rb') as f:
#         SVC_Model = pickle.load(f)
# except FileNotFoundError:
#     st.error("SVC model file not found. Please ensure 'svc_model.pkl' is in the same directory.")
#     st.stop()
# 
# 
# # --- Streamlit App ---
# st.title('Sentiment Prediction App')
# 
# # Input fields
# title_input = st.text_input('Enter the title:')
# body_input = st.text_area('Enter the body of the review:')
# rating_input = st.selectbox('Select the rating (1-5):', options=[1, 2, 3, 4, 5])
# 
# # Prediction button
# if st.button('Predict Sentiment'):
#     # This block will contain the prediction logic in the next step
#     pass

"""## Integrate model

### Subtask:
Load the trained SVC model and the TF-IDF vectorizer within the Streamlit app.

## Implement prediction logic

### Subtask:
Add the code to preprocess the input, vectorize it using the loaded TF-IDF vectorizer, and make predictions using the loaded SVC model.

**Reasoning**:
Create a pandas DataFrame from the user inputs, map the rating, clean the text columns, combine them, vectorize the combined text, and predict the sentiment using the loaded model.
"""

# Create DataFrame from inputs
    input_df = pd.DataFrame({
        'title': [title_input],
        'body': [body_input],
        'rating': [rating_input]
    })

    # Map rating back to categorical
    rating_map_inv = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
    input_df['rating'] = input_df['rating'].map(rating_map_inv)

    # Clean title and body
    input_df['title'] = vectorized_clean_series(input_df['title'])
    input_df['body'] = vectorized_clean_series(input_df['body'])

    # Combine cleaned text and mapped rating
    input_df['combined'] = input_df['title'] + input_df['rating'] + input_df['body']

    # Vectorize the combined text
    input_vectorized = tfidf.transform(input_df['combined'])

    # Predict sentiment
    predicted_sentiment = SVC_Model.predict(input_vectorized)[0]

    # Display the prediction
    st.write(f'Predicted Sentiment: {predicted_sentiment}')

"""**Reasoning**:
The error `NameError: name 'title_input' is not defined` indicates that the variables `title_input`, `body_input`, and `rating_input` are not accessible in the current scope. This is because the code block was executed outside the `if st.button('Predict Sentiment'):` block where these variables are defined in the Streamlit app code. The entire prediction logic needs to be placed inside this conditional block.


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import SVC
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import pickle
# import streamlit as st
# 
# # --- Data Cleaning Code ---
# hindi_noise_words = [
#     'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
#     'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
#     'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
#     'kya', 'tera', 'mera', 'diya', 'diye', 'de'
# ]
# 
# romanized_mapping = {
#     'accha': 'good',
#     'acha': 'good',
#     'achha': 'good',
#     'bahut': 'very',
#     'bhut': 'very',
#     'kharab': 'bad',
#     'bekar': 'bad',
#     'sahi': 'correct',
#     'nhi': 'not',
#     'nahi': 'not',
#     'milega': 'receive',
#     'mila': 'received',
#     'liya': 'take',
#     'diya': 'give',
#     'lene': 'take',
#     'achcha':'good',
#     'achhi':'good',
#     'achchaa':'good',
#     'acchi':'good',
#     'aap':'you',
#     'bakwaas':'bad',
#     'bakwas':'bad',
#     'bada':'big'
# }
# 
# # Download resources only if they haven't been downloaded
# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt_tab')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt_tab', quiet=True)
# 
# 
# english_stopwords = set(stopwords.words('english'))
# stop_words = english_stopwords.union(set(hindi_noise_words))
# 
# def vectorized_clean_series(s: pd.Series):
#     s = s.fillna('').astype(str)
#     s = s.str.normalize('NFKC')
#     s = s.str.lower()
# 
#     s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
#     s = s.str.replace(r'\S+@\S+', ' ', regex=True)
#     s = s.str.replace(r'<.*?>', ' ', regex=True)
#     s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)
# 
#     def replace_romanized_words(text):
#         if not text:
#             return ""
#         words = text.split()
#         words = [romanized_mapping.get(word, word) for word in words]
#         return ' '.join(words)
# 
#     s = s.apply(replace_romanized_words)
# 
#     s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
#     s = s.str.replace(r'\s+', ' ', regex=True).str.strip()
# 
#     s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))
# 
#     return s
# 
# # --- VADER Sentiment Analysis ---
# analyzer = SentimentIntensityAnalyzer()
# 
# def get_vader_sentiment_category(text):
#     if not isinstance(text, str):
#         return "Neutral"
#     vs = analyzer.polarity_scores(text)
#     compound_score = vs['compound']
#     if compound_score >= 0.05:
#         return "Positive"
#     elif compound_score <= -0.05:
#         return "Negative"
#     else:
#         return "Neutral"
# 
# # --- Model and Vectorization Loading ---
# # Load the pre-trained TF-IDF vectorizer and SVC model
# # In a real application, you would train and save these first.
# try:
#     with open('tfidf_vectorizer.pkl', 'rb') as f:
#         tfidf = pickle.load(f)
# except FileNotFoundError:
#     st.error("TF-IDF vectorizer file not found. Please ensure 'tfidf_vectorizer.pkl' is in the same directory.")
#     st.stop()
# 
# try:
#     with open('svc_model.pkl', 'rb') as f:
#         SVC_Model = pickle.load(f)
# except FileNotFoundError:
#     st.error("SVC model file not found. Please ensure 'svc_model.pkl' is in the same directory.")
#     st.stop()
# 
# 
# # --- Streamlit App ---
# st.title('Sentiment Prediction App')
# 
# # Input fields
# title_input = st.text_input('Enter the title:')
# body_input = st.text_area('Enter the body of the review:')
# rating_input = st.selectbox('Select the rating (1-5):', options=[1, 2, 3, 4, 5])
# 
# # Prediction button
# if st.button('Predict Sentiment'):
#     # Create DataFrame from inputs
#     input_df = pd.DataFrame({
#         'title': [title_input],
#         'body': [body_input],
#         'rating': [rating_input]
#     })
# 
#     # Map rating back to categorical
#     rating_map_inv = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
#     input_df['rating'] = input_df['rating'].map(rating_map_inv)
# 
#     # Clean title and body
#     input_df['title'] = vectorized_clean_series(input_df['title'])
#     input_df['body'] = vectorized_clean_series(input_df['body'])
# 
#     # Combine cleaned text and mapped rating
#     input_df['combined'] = input_df['title'] + input_df['rating'] + input_df['body']
# 
#     # Vectorize the combined text
#     input_vectorized = tfidf.transform(input_df['combined'])
# 
#     # Predict sentiment
#     predicted_sentiment = SVC_Model.predict(input_vectorized)[0]
# 
#     # Display the prediction
#     st.write(f'Predicted Sentiment: {predicted_sentiment}')

"""## Display results

### Subtask:
Show the predicted sentiment category (Positive, Negative, Neutral) and the sentiment percentages in the Streamlit app.

**Reasoning**:
Calculate and display the sentiment percentages using VADER and display the predicted sentiment category.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import SVC
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import pickle
# import streamlit as st
# 
# # --- Data Cleaning Code ---
# hindi_noise_words = [
#     'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
#     'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
#     'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
#     'kya', 'tera', 'mera', 'diya', 'diye', 'de'
# ]
# 
# romanized_mapping = {
#     'accha': 'good',
#     'acha': 'good',
#     'achha': 'good',
#     'bahut': 'very',
#     'bhut': 'very',
#     'kharab': 'bad',
#     'bekar': 'bad',
#     'sahi': 'correct',
#     'nhi': 'not',
#     'nahi': 'not',
#     'milega': 'receive',
#     'mila': 'received',
#     'liya': 'take',
#     'diya': 'give',
#     'lene': 'take',
#     'achcha':'good',
#     'achhi':'good',
#     'achchaa':'good',
#     'acchi':'good',
#     'aap':'you',
#     'bakwaas':'bad',
#     'bakwas':'bad',
#     'bada':'big'
# }
# 
# # Download resources only if they haven't been downloaded
# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt_tab')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt_tab', quiet=True)
# 
# 
# english_stopwords = set(stopwords.words('english'))
# stop_words = english_stopwords.union(set(hindi_noise_words))
# 
# def vectorized_clean_series(s: pd.Series):
#     s = s.fillna('').astype(str)
#     s = s.str.normalize('NFKC')
#     s = s.str.lower()
# 
#     s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
#     s = s.str.replace(r'\S+@\S+', ' ', regex=True)
#     s = s.str.replace(r'<.*?>', ' ', regex=True)
#     s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)
# 
#     def replace_romanized_words(text):
#         if not text:
#             return ""
#         words = text.split()
#         words = [romanized_mapping.get(word, word) for word in words]
#         return ' '.join(words)
# 
#     s = s.apply(replace_romanized_words)
# 
#     s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
#     s = s.str.replace(r'\s+', ' ', regex=True).str.strip()
# 
#     s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))
# 
#     return s
# 
# # --- VADER Sentiment Analysis ---
# analyzer = SentimentIntensityAnalyzer()
# 
# def get_vader_sentiment_category(text):
#     if not isinstance(text, str):
#         return "Neutral"
#     vs = analyzer.polarity_scores(text)
#     compound_score = vs['compound']
#     if compound_score >= 0.05:
#         return "Positive"
#     elif compound_score <= -0.05:
#         return "Negative"
#     else:
#         return "Neutral"
# 
# # --- Model and Vectorization Loading ---
# # Load the pre-trained TF-IDF vectorizer and SVC model
# # In a real application, you would train and save these first.
# try:
#     with open('tfidf_vectorizer.pkl', 'rb') as f:
#         tfidf = pickle.load(f)
# except FileNotFoundError:
#     st.error("TF-IDF vectorizer file not found. Please ensure 'tfidf_vectorizer.pkl' is in the same directory.")
#     st.stop()
# 
# try:
#     with open('svc_model.pkl', 'rb') as f:
#         SVC_Model = pickle.load(f)
# except FileNotFoundError:
#     st.error("SVC model file not found. Please ensure 'svc_model.pkl' is in the same directory.")
#     st.stop()
# 
# 
# # --- Streamlit App ---
# st.title('Sentiment Prediction App')
# 
# # Input fields
# title_input = st.text_input('Enter the title:')
# body_input = st.text_area('Enter the body of the review:')
# rating_input = st.selectbox('Select the rating (1-5):', options=[1, 2, 3, 4, 5])
# 
# # Prediction button
# if st.button('Predict Sentiment'):
#     # Create DataFrame from inputs
#     input_df = pd.DataFrame({
#         'title': [title_input],
#         'body': [body_input],
#         'rating': [rating_input]
#     })
# 
#     # Map rating back to categorical
#     rating_map_inv = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
#     input_df['rating'] = input_df['rating'].map(rating_map_inv)
# 
#     # Clean title and body
#     input_df['title'] = vectorized_clean_series(input_df['title'])
#     input_df['body'] = vectorized_clean_series(input_df['body'])
# 
#     # Combine cleaned text and mapped rating
#     input_df['combined'] = input_df['title'] + input_df['rating'] + input_df['body']
# 
#     # Vectorize the combined text
#     input_vectorized = tfidf.transform(input_df['combined'])
# 
#     # Predict sentiment using the SVC model
#     predicted_sentiment_category = SVC_Model.predict(input_vectorized)[0]
# 
#     # Calculate sentiment percentages using VADER on the original body text
#     vader_scores = analyzer.polarity_scores(body_input)
#     negative_percentage = vader_scores['neg'] * 100
#     neutral_percentage = vader_scores['neu'] * 100
#     positive_percentage = vader_scores['pos'] * 100
# 
#     # Display the predicted sentiment category
#     st.write(f'Predicted Sentiment Category: {predicted_sentiment_category}')
# 
#     # Display sentiment percentages from VADER
#     st.write('Sentiment Percentages (VADER):')
#     st.write(f'- Negative: {negative_percentage:.2f}%')
#     st.write(f'- Neutral: {neutral_percentage:.2f}%')
#     st.write(f'- Positive: {positive_percentage:.2f}%')

"""## Save as .py file

### Subtask:
Save the complete code as a Python file (e.g., `app.py`).

**Reasoning**:
Save the complete Streamlit application code to `app.py`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# import re
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import SVC
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import pickle
# import streamlit as st
# 
# # --- Data Cleaning Code ---
# hindi_noise_words = [
#     'ka', 'ki', 'ke', 'aur', 'hai', 'main', 'mai', 'bhi', 'yeh', 'ye',
#     'ko', 'mein', 'me', 'to', 'ho', 'hona', 'tha', 'thi', 'the',
#     'nhi', 'fir', 'phir', 'lekin', 'par', 'apne', 'usko', 'kuch', 'hoga',
#     'kya', 'tera', 'mera', 'diya', 'diye', 'de'
# ]
# 
# romanized_mapping = {
#     'accha': 'good',
#     'acha': 'good',
#     'achha': 'good',
#     'bahut': 'very',
#     'bhut': 'very',
#     'kharab': 'bad',
#     'bekar': 'bad',
#     'sahi': 'correct',
#     'nhi': 'not',
#     'nahi': 'not',
#     'milega': 'receive',
#     'mila': 'received',
#     'liya': 'take',
#     'diya': 'give',
#     'lene': 'take',
#     'achcha':'good',
#     'achhi':'good',
#     'achchaa':'good',
#     'acchi':'good',
#     'aap':'you',
#     'bakwaas':'bad',
#     'bakwas':'bad',
#     'bada':'big'
# }
# 
# # Download resources only if they haven't been downloaded
# try:
#     nltk.data.find('corpora/stopwords')
# except nltk.downloader.DownloadError:
#     nltk.download('stopwords', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt', quiet=True)
# try:
#     nltk.data.find('tokenizers/punkt_tab')
# except nltk.downloader.DownloadError:
#     nltk.download('punkt_tab', quiet=True)
# 
# 
# english_stopwords = set(stopwords.words('english'))
# stop_words = english_stopwords.union(set(hindi_noise_words))
# 
# def vectorized_clean_series(s: pd.Series):
#     s = s.fillna('').astype(str)
#     s = s.str.normalize('NFKC')
#     s = s.str.lower()
# 
#     s = s.str.replace(r'http\S+|www\.\S+', ' ', regex=True)
#     s = s.str.replace(r'\S+@\S+', ' ', regex=True)
#     s = s.str.replace(r'<.*?>', ' ', regex=True)
#     s = s.str.replace(r'\d+(?:[.,]\d+)*', ' ', regex=True)
# 
#     def replace_romanized_words(text):
#         if not text:
#             return ""
#         words = text.split()
#         words = [romanized_mapping.get(word, word) for word in words]
#         return ' '.join(words)
# 
#     s = s.apply(replace_romanized_words)
# 
#     s = s.str.replace(r'[^a-z\s]', ' ', regex=True)
#     s = s.str.replace(r'\s+', ' ', regex=True).str.strip()
# 
#     s = s.apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words and word]))
# 
#     return s
# 
# # --- VADER Sentiment Analysis ---
# analyzer = SentimentIntensityAnalyzer()
# 
# def get_vader_sentiment_category(text):
#     if not isinstance(text, str):
#         return "Neutral"
#     vs = analyzer.polarity_scores(text)
#     compound_score = vs['compound']
#     if compound_score >= 0.05:
#         return "Positive"
#     elif compound_score <= -0.05:
#         return "Negative"
#     else:
#         return "Neutral"
# 
# # --- Model and Vectorization Loading ---
# # Load the pre-trained TF-IDF vectorizer and SVC model
# # In a real application, you would train and save these first.
# try:
#     with open('tfidf_vectorizer.pkl', 'rb') as f:
#         tfidf = pickle.load(f)
# except FileNotFoundError:
#     st.error("TF-IDF vectorizer file not found. Please ensure 'tfidf_vectorizer.pkl' is in the same directory.")
#     st.stop()
# 
# try:
#     with open('svc_model.pkl', 'rb') as f:
#         SVC_Model = pickle.load(f)
# except FileNotFoundError:
#     st.error("SVC model file not found. Please ensure 'svc_model.pkl' is in the same directory.")
#     st.stop()
# 
# 
# # --- Streamlit App ---
# st.title('Sentiment Prediction App')
# 
# # Input fields
# title_input = st.text_input('Enter the title:')
# body_input = st.text_area('Enter the body of the review:')
# rating_input = st.selectbox('Select the rating (1-5):', options=[1, 2, 3, 4, 5])
# 
# # Prediction button
# if st.button('Predict Sentiment'):
#     # Create DataFrame from inputs
#     input_df = pd.DataFrame({
#         'title': [title_input],
#         'body': [body_input],
#         'rating': [rating_input]
#     })
# 
#     # Map rating back to categorical
#     rating_map_inv = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
#     input_df['rating'] = input_df['rating'].map(rating_map_inv)
# 
#     # Clean title and body
#     input_df['title'] = vectorized_clean_series(input_df['title'])
#     input_df['body'] = vectorized_clean_series(input_df['body'])
# 
#     # Combine cleaned text and mapped rating
#     input_df['combined'] = input_df['title'] + input_df['rating'] + input_df['body']
# 
#     # Vectorize the combined text
#     input_vectorized = tfidf.transform(input_df['combined'])
# 
#     # Predict sentiment using the SVC model
#     predicted_sentiment_category = SVC_Model.predict(input_vectorized)[0]
# 
#     # Calculate sentiment percentages using VADER on the original body text
#     vader_scores = analyzer.polarity_scores(body_input)
#     negative_percentage = vader_scores['neg'] * 100
#     neutral_percentage = vader_scores['neu'] * 100
#     positive_percentage = vader_scores['pos'] * 100
# 
#     # Display the predicted sentiment category
#     st.write(f'Predicted Sentiment Category: {predicted_sentiment_category}')
# 
#     # Display sentiment percentages from VADER
#     st.write('Sentiment Percentages (VADER):')
#     st.write(f'- Negative: {negative_percentage:.2f}%')
#     st.write(f'- Neutral: {neutral_percentage:.2f}%')
#     st.write(f'- Positive: {positive_percentage:.2f}%')

"""## Summary:

### Data Analysis Key Findings

*   The process involved extracting code for data cleaning, vectorization, and model inference from a conceptual notebook structure and integrating it into a Streamlit application script (`app.py`).
*   A custom data cleaning function `vectorized_clean_series` was implemented to handle text normalization, removal of URLs, emails, HTML tags, numbers, special characters, and stopwords (including Hindi noise words and Romanized mappings).
*   VADER sentiment analysis was incorporated to calculate sentiment percentages (negative, neutral, positive) based on the raw body text input.
*   The Streamlit application includes input fields for title, body, and rating, a button to trigger the prediction, and displays both the predicted sentiment category (from an SVC model) and the VADER sentiment percentages.
*   Placeholder code and error handling for loading a pre-trained TF-IDF vectorizer and SVC model from pickle files (`tfidf_vectorizer.pkl` and `svc_model.pkl`) were included, with dummy files created to facilitate the process.
*   The prediction logic involves creating a pandas DataFrame from the inputs, mapping the rating to a categorical value, cleaning the text, combining the cleaned text and mapped rating, vectorizing the combined text using the loaded TF-IDF vectorizer, and finally using the loaded SVC model to predict the sentiment category.

### Insights or Next Steps

*   Ensure that properly trained `tfidf_vectorizer.pkl` and `svc_model.pkl` files are available in the same directory as `app.py` for the Streamlit application to function correctly.
*   Consider adding input validation and error handling in the Streamlit app to provide better feedback to the user if inputs are missing or invalid.

"""